<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Work - Vivek Chinimilli</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600;700&family=Lexend:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="grain-overlay"></div>

    <nav class="nav">
        <div class="nav-content">
            <div class="logo"><a href="index.html">VC</a></div>
            <div class="nav-links">
                <a href="index.html">home</a>
                <a href="about.html">about</a>
                <a href="work.html" class="active">work</a>
                <a href="media.html">media</a>
                <a href="bookshelf.html">bookshelf</a>
                <a href="projects.html">projects</a>
                <a href="now.html">now</a>
            </div>
        </div>
    </nav>

    <main>
        <section class="hero">
            <div class="hero-content">
                <div class="hero-label">work</div>
                <h1 class="hero-title">
                    where i've worked
                </h1>
                <p class="hero-description">
                    product analyst specializing in risk modeling, portfolio analytics, and scaling fintech products.
                    currently working on cash advance infrastructure serving millions of users.
                </p>
                <div class="hero-stats">
                    <div class="stat">
                        <div class="stat-value">$2b</div>
                        <div class="stat-label">target annual volume</div>
                    </div>
                    <div class="stat">
                        <div class="stat-value">sql + python</div>
                        <div class="stat-label">core stack</div>
                    </div>
                    <div class="stat">
                        <div class="stat-value">fintech</div>
                        <div class="stat-label">domain focus</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="projects" class="projects">
            <div class="section-header">
                <h2 class="section-title">projects</h2>
                <div class="section-line"></div>
            </div>

            <div class="projects-grid">
                <article class="project-card featured">
                    <div class="project-header">
                        <div class="project-label">onepay • risk analytics</div>
                        <div class="project-tech">
                            <span>sql</span>
                            <span>python</span>
                            <span>risk modeling</span>
                        </div>
                    </div>
                    <h3 class="project-title">overdraft risk model migration</h3>
                    <p class="project-description">
                        led migration from v2 to v3 risk model for 1m user overdraft program ($60m exposure). the premise
                        was straightforward: newer model should perform better. extensive backtesting revealed something
                        unexpected—v3 was essentially a scaled-down version of v2, just more conservative across the board
                        without actually improving risk separation. this kicked off a different project: building custom
                        segmentation that better captured credit-neediness signals the existing models were missing.
                        <br><br>
                        the interesting work was in the rebinning analysis. standard approach would be to validate the new
                        model and deploy it. instead, we tested which pockets of declined users were actually low-risk,
                        what behavioral patterns the model was over-penalizing, and where the existing score cutoffs were
                        leaving opportunity on the table. turned out credit-neediness wasn't linear—users in middle risk
                        bands were more sensitive to timing and cash flow volatility than absolute balance levels, which
                        neither model captured well.
                        <br><br>
                        the validation process became more valuable than the migration itself. developed a framework for
                        comparing model populations that we still use for evaluating new risk models—looking at actual
                        user cohorts rather than aggregate performance metrics, which tend to hide the segments where
                        models break down.
                    </p>
                    <div class="project-highlights">
                        <div class="highlight">model validation & comparison</div>
                        <div class="highlight">performance optimization</div>
                        <div class="highlight">executive reporting</div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-header">
                        <div class="project-label">onepay • product analytics</div>
                        <div class="project-tech">
                            <span>python</span>
                            <span>sql</span>
                            <span>credit policy</span>
                        </div>
                    </div>
                    <h3 class="project-title">onepay advance credit policy</h3>
                    <p class="project-description">
                        built the credit policy from scratch for a new cash advance product with no historical data. the
                        core challenge: we had rich overdraft data but zero advance performance data, so every decision
                        was a hypothesis. started with backtesting every behavioral and balance-based signal we could
                        think of—transaction frequency, deposit consistency, account tenure, nsf history—to see what
                        actually split risk in overdraft.
                        <br><br>
                        settled on a 3-tier segmentation model, but the framework came from testing rather than intuition.
                        tested whether users who managed overdraft well would manage advances well (mostly true), whether
                        balance volatility predicted advance default (not really), and whether we could use overdraft
                        limits as a proxy for advance limits (true for behavior, broke down for balances). each hypothesis
                        required different data cuts and different validation approaches.
                        <br><br>
                        the interesting tension was between building a conservative initial policy and leaving room to
                        learn from real data. we set exposure limits low initially, then built in systematic review points
                        where we'd compare actual performance to predictions and adjust rules. the behavioral patterns
                        held up better than expected, but some of our balance-based assumptions were wrong—higher overdraft
                        users were actually *less* likely to default on advances, which forced a policy rewrite three
                        months in.
                        <br><br>
                        what made this work was treating the policy as a learning system rather than a static rule set.
                        every cohort we approved became a test of our assumptions, and we built the infrastructure to
                        measure that continuously.
                    </p>
                    <div class="project-highlights">
                        <div class="highlight">behavioral segmentation</div>
                        <div class="highlight">churn prediction</div>
                        <div class="highlight">growth modeling</div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-header">
                        <div class="project-label">onepay • infrastructure</div>
                        <div class="project-tech">
                            <span>python</span>
                            <span>algorithms</span>
                            <span>prediction</span>
                        </div>
                    </div>
                    <h3 class="project-title">paycheck prediction system</h3>
                    <p class="project-description">
                        built a system to predict when 400k users would receive their next paycheck. sounds straightforward—
                        people get paid on a schedule—but the data quality problem was brutal. direct deposit timestamps
                        were messy and inconsistent (timezone issues, weekend delays, early/late payments), and we had no
                        reliable "ground truth" for actual pay schedules.
                        <br><br>
                        started with the simplest possible approach: calculate historical cadence between paychecks, use
                        last observed payday, predict forward. this worked for about 60% of users—the ones with clean
                        biweekly or monthly schedules. the other 40% were edge cases that each required different logic:
                        holidays shifting pay dates, irregular schedules for gig workers, semi-monthly vs. biweekly
                        confusion, users who switched jobs mid-stream.
                        <br><br>
                        the interesting part was systematically working through these edge cases. built a classification
                        system to identify users with irregular patterns, then developed separate prediction logic for
                        each type. gig workers needed rolling averages rather than fixed cadences. semi-monthly employees
                        needed calendar-based logic (15th and end-of-month) rather than day-counting. job switchers needed
                        a "reset" mechanism to throw out old patterns.
                        <br><br>
                        the algorithm ended up being less elegant than i wanted—lots of conditional branching and special
                        cases—but that's what the data required. the clean mathematical approach failed in production
                        because payroll systems are messy and human schedules don't follow patterns. got to 98% accuracy
                        by embracing that messiness rather than trying to force it into a unified framework.
                    </p>
                    <div class="project-highlights">
                        <div class="highlight">time series prediction</div>
                        <div class="highlight">feature engineering</div>
                        <div class="highlight">production deployment</div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-header">
                        <div class="project-label">capital one • strategy & operations</div>
                        <div class="project-tech">
                            <span>python</span>
                            <span>sql</span>
                            <span>a/b testing</span>
                        </div>
                    </div>
                    <h3 class="project-title">williams sonoma card transformation</h3>
                    <p class="project-description">
                        led transition of store-only cardholders to updated cobrand with spend-anywhere capability. the
                        core tension: existing cardholders were a riskier population (selected for store-only product,
                        lower engagement), but product expansion would increase utility and potentially improve retention.
                        <br><br>
                        developed segmentation to identify which users could safely receive broader spending power. tested
                        different risk signals—payment history, utilization patterns, tenure—to see what actually predicted
                        whether users would maintain good standing with expanded access. the interesting finding: risk
                        wasn't the binding constraint we thought it was. engagement patterns were more predictive than
                        credit behavior. users who actively used the store card, even with higher utilization, were
                        actually safer bets for expansion than low-utilization cardholders.
                        <br><br>
                        coordinated phased rollout across product, risk, and marketing. each phase was an experiment—we'd
                        upgrade a cohort, monitor performance for 60 days, adjust segmentation rules, then expand. the
                        monitoring framework became more valuable than the initial segmentation. built real-time dashboards
                        tracking early warning signals (utilization spikes, missed payments, account closures) that let us
                        catch problems before they became portfolio-level issues.
                    </p>
                    <div class="project-highlights">
                        <div class="highlight">real-time monitoring</div>
                        <div class="highlight">anomaly detection</div>
                        <div class="highlight">executive dashboards</div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-header">
                        <div class="project-label">capital one • credit policy</div>
                        <div class="project-tech">
                            <span>r</span>
                            <span>fraud modeling</span>
                            <span>risk analysis</span>
                        </div>
                    </div>
                    <h3 class="project-title">kohl's fraud model implementation</h3>
                    <p class="project-description">
                        evaluated new fraud model using swap-in/swap-out analysis—comparing credit performance of approvals
                        under new model vs. old model to quantify expected impact before implementation. the methodology
                        was more interesting than the results: rather than looking at score distributions or approval rate
                        shifts, we compared actual populations of approved applicants to see how risk profiles would change.
                        <br><br>
                        built the analysis in r, segmenting applicants into "approved by both," "approved only by old model,"
                        and "approved only by new model." the last two groups were the interesting ones—they showed where
                        the models disagreed and let us estimate the portfolio impact of those disagreements. turned out
                        the new model was catching fraud patterns the old one missed but was also being more conservative
                        on legitimate applicants in certain segments.
                        <br><br>
                        after rollout, built monitoring framework to validate predictions and track ongoing performance.
                        this became the template for future model validations at capital one—comparing actual approval
                        populations rather than hypothetical score distributions was far more predictive of real-world
                        performance. the validation methodology ended up being reused across multiple product lines.
                    </p>
                    <div class="project-highlights">
                        <div class="highlight">risk-return analysis</div>
                        <div class="highlight">cohort comparison</div>
                        <div class="highlight">strategy optimization</div>
                    </div>
                </article>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-text">
                <p>built with curiosity and attention to detail.</p>
            </div>
            <div class="footer-links">
                <a href="mailto:vivekc691@gmail.com">email</a>
                <a href="https://github.com/vivekc21" target="_blank">github</a>
                <a href="https://www.linkedin.com/in/vivekchinimilli/" target="_blank">linkedin</a>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
